# 本地推理开关配置说明 🎛️

## 📋 两种配置方式

### 方式1：管理后台配置（运行时配置）

**访问**：`http://123.57.68.4:8000/` → "⚙️ 配置管理" → "🤖 推理策略配置"

**特点**：
- ✅ **立即生效**：修改后无需重启服务，下一个请求立即使用新配置
- ❌ **不持久化**：服务重启后配置丢失，恢复为 `.env` 文件中的值

**工作原理**：
```javascript
// 管理后台修改配置
勾选"使用本地推理" → 调用API → settings.USE_LOCAL_INFERENCE = true
                                    ↓
                            立即生效！下一个请求就用小模型
```

**适用场景**：
- 临时测试小模型推理
- 快速切换推理方式
- 调试和对比效果

---

### 方式2：`.env` 文件配置（持久化配置）

**位置**：`/opt/ImageClassifierBackend/.env`

**编辑方式**：
```bash
# 1. 编辑 .env 文件
ssh root@123.57.68.4 "vi /opt/ImageClassifierBackend/.env"

# 或者直接添加配置
ssh root@123.57.68.4 "echo 'USE_LOCAL_INFERENCE=true' >> /opt/ImageClassifierBackend/.env"

# 2. 重启服务
ssh root@123.57.68.4 "systemctl restart image-classifier"
```

**特点**：
- ✅ **持久化**：服务重启后配置保留
- ❌ **需要重启**：修改后必须重启服务才能生效

**工作原理**：
```bash
.env 文件: USE_LOCAL_INFERENCE=true
    ↓ 服务启动时读取
settings.USE_LOCAL_INFERENCE = true
    ↓
所有请求使用小模型推理
    ↓ 重启服务
settings.USE_LOCAL_INFERENCE = true （从 .env 重新读取，配置保留）
```

**适用场景**：
- 生产环境的正式配置
- 长期使用某种推理方式
- 确保重启后配置不丢失

---

## 🔄 配置优先级和覆盖关系

### 加载顺序

```
1. 服务启动
   ↓
   读取 .env 文件
   ↓
   settings.USE_LOCAL_INFERENCE = true  （假设 .env 中是 true）
   
2. 运行期间，管理后台修改
   ↓
   settings.USE_LOCAL_INFERENCE = false  （运行时修改）
   ↓
   立即生效，使用大模型
   
3. 服务重启
   ↓
   重新读取 .env 文件
   ↓
   settings.USE_LOCAL_INFERENCE = true   （恢复为 .env 中的值）
   ↓
   管理后台的修改丢失！
```

---

## 🎯 两个配置项说明

### 1. USE_LOCAL_INFERENCE（使用本地推理）

**作用**：控制是否强制使用小模型推理

**取值**：
- `true` - 强制使用小模型，不调用大模型
- `false` - 优先使用大模型

**示例**：
```bash
# .env 文件
USE_LOCAL_INFERENCE=true   # 强制使用小模型
```

**效果**：
```python
if settings.USE_LOCAL_INFERENCE:
    # 直接使用小模型推理
    result = await local_inference.classify_image(image_bytes)
    return {
        "category": "",  # 空字符串，客户端需要映射
        "local_inference_result": {...}
    }
else:
    # 调用大模型
    result = await model_client.classify_image(image_bytes)
    return {
        "category": "foods",  # 有值，客户端直接使用
    }
```

---

### 2. LOCAL_INFERENCE_FALLBACK（降级策略）

**作用**：大模型失败时是否降级到小模型

**取值**：
- `true` - 大模型失败时自动降级到小模型
- `false` - 大模型失败时直接返回错误

**示例**：
```bash
# .env 文件
LOCAL_INFERENCE_FALLBACK=true   # 开启降级
```

**效果**：
```python
try:
    # 调用大模型
    result = await model_client.classify_image(image_bytes)
except Exception as e:
    if settings.LOCAL_INFERENCE_FALLBACK:
        # 降级到小模型
        result = await local_inference.classify_image(image_bytes)
        return {
            "category": "",  # 空字符串
            "local_inference_result": {...}
        }
    else:
        # 直接抛出错误
        raise
```

---

## 📊 四种配置组合

| USE_LOCAL_INFERENCE | LOCAL_INFERENCE_FALLBACK | 行为 |
|-------------------|------------------------|------|
| **false** | **false** | 只用大模型，失败就报错 ❌ |
| **false** | **true** | 优先大模型，失败降级小模型 ✅ **推荐生产** |
| **true** | **false** | 只用小模型，失败就报错 |
| **true** | **true** | 只用小模型（降级配置无意义） |

---

## 🛠️ 当前配置状态

### `.env` 文件（持久化配置）

```bash
USE_LOCAL_INFERENCE=true           # ✅ 强制使用小模型
LOCAL_INFERENCE_FALLBACK=true      # ✅ 降级策略（但不会触发，因为不调用大模型）
```

### 运行时配置（管理后台）

你可以在管理后台实时查看和修改：
1. 访问 `http://123.57.68.4:8000/`
2. 登录（用户名：zywl）
3. 进入 "⚙️ 配置管理"
4. 查看 "🤖 推理策略配置"

**当前状态**：
- 🤖 使用本地推理：✅ **已开启**（从 .env 加载）
- 🛡️ 大模型失败时降级：✅ 已开启

---

## 💡 推荐配置策略

### 场景1：生产环境 - 高可用优先

**`.env` 文件配置**：
```bash
USE_LOCAL_INFERENCE=false          # 优先大模型
LOCAL_INFERENCE_FALLBACK=true      # 失败时降级
```

**效果**：
- 正常情况：使用大模型（准确度高）
- 大模型故障：自动降级小模型（保证可用）

---

### 场景2：生产环境 - 速度和成本优先

**`.env` 文件配置**：
```bash
USE_LOCAL_INFERENCE=true           # 强制小模型
LOCAL_INFERENCE_FALLBACK=true      # （不影响）
```

**效果**：
- 所有请求使用小模型（快速、无费用）
- 客户端需要实现分类映射逻辑

---

### 场景3：测试环境 - 灵活切换

**`.env` 文件配置**：
```bash
USE_LOCAL_INFERENCE=false          # 默认大模型
LOCAL_INFERENCE_FALLBACK=true      # 降级策略
```

**管理后台操作**：
- 需要测试小模型时 → 勾选"使用本地推理" → 立即生效
- 测试完成后 → 取消勾选 → 恢复大模型
- 重启服务 → 自动恢复默认配置（大模型）

---

## ⚠️ 重要提示

### 1. 管理后台修改不会写入 .env

**管理后台修改后重启会丢失**：
```
管理后台开启本地推理 → 立即生效 → 服务重启 → ❌ 配置丢失
```

### 2. 如需持久化，必须修改 .env

**正确做法**：
```bash
# 1. 管理后台测试配置效果
管理后台勾选"使用本地推理" → 测试几次请求 → 效果满意

# 2. 将配置写入 .env 文件
ssh root@123.57.68.4 "echo 'USE_LOCAL_INFERENCE=true' >> /opt/ImageClassifierBackend/.env"

# 3. 重启服务
ssh root@123.57.68.4 "systemctl restart image-classifier"

# ✅ 现在重启后配置不会丢失
```

---

## 🔍 如何验证当前配置

### 方法1：查看 .env 文件

```bash
ssh root@123.57.68.4 "cat /opt/ImageClassifierBackend/.env | grep USE_LOCAL"
```

**输出**：
```
USE_LOCAL_INFERENCE=true
```

### 方法2：测试分类请求

从客户端上传图片，查看返回结果：

**使用小模型**：
```json
{
  "data": {
    "category": "",  // ← 空字符串
    "local_inference_result": {...}
  }
}
```

**使用大模型**：
```json
{
  "data": {
    "category": "foods",  // ← 有值
    "confidence": 0.92
  }
}
```

### 方法3：查看服务日志

```bash
ssh root@123.57.68.4 "journalctl -u image-classifier | tail -20 | grep '方式:'"
```

**输出**：
```
分类完成 [req_xxx]: local_pending (342ms) [方式: local]     ← 小模型
分类完成 [req_xxx]: foods (996ms) [方式: llm]              ← 大模型
```

---

## 📝 总结

### 配置控制方式

| 问题 | 答案 |
|------|------|
| **管理后台修改能立即生效吗？** | ✅ 是的，无需重启 |
| **管理后台修改会持久化吗？** | ❌ 不会，重启后丢失 |
| **重启后会用什么配置？** | 使用 `.env` 文件中的配置 |
| **如何让配置永久生效？** | 修改 `.env` 文件并重启服务 |

### 最佳实践

1. **开发/测试**：使用管理后台快速切换测试
2. **确定配置**：写入 `.env` 文件
3. **生产部署**：确保 `.env` 文件包含正确配置

---

**当前配置**：`.env` 文件已设置 `USE_LOCAL_INFERENCE=true`，重启后不会丢失！✅

