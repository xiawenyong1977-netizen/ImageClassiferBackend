# 混合推理策略说明 🔄

## 🎯 功能概述

已实现**混合推理策略**，支持大模型和本地ONNX模型的智能切换和降级。

## 🆕 新增功能

### 1. 配置开关

在 `.env` 文件中配置：

```bash
# 是否使用本地推理（开启后不调用大模型）
USE_LOCAL_INFERENCE=false

# 大模型失败时是否降级到本地推理（推荐开启）
LOCAL_INFERENCE_FALLBACK=true
```

### 2. 推理策略

#### 策略A：优先本地推理（`USE_LOCAL_INFERENCE=true`）

```
请求 → 检查缓存
  ├─ 缓存命中 → 返回缓存结果
  └─ 缓存未命中 → 使用本地推理
      ├─ 本地推理成功 → 返回结果（含原始检测数据）
      └─ 本地推理失败 → 降级到大模型（如果开启 LOCAL_INFERENCE_FALLBACK）
```

#### 策略B：优先大模型（`USE_LOCAL_INFERENCE=false`，默认）

```
请求 → 检查缓存
  ├─ 缓存命中 → 返回缓存结果
  └─ 缓存未命中 → 调用大模型
      ├─ 大模型成功 → 返回结果并缓存
      └─ 大模型失败 → 降级到本地推理（如果开启 LOCAL_INFERENCE_FALLBACK）
```

## 📊 推理方式对比

| 方式 | 速度 | 成本 | 准确性 | 说明 |
|------|------|------|--------|------|
| **大模型** | 2-5秒 | ￥0.01/次 | ⭐⭐⭐⭐⭐ | 高准确性，需API费用 |
| **本地推理** | 0.1-0.5秒 | 免费 | ⭐⭐⭐ | 快速免费，需客户端映射 |
| **缓存** | < 0.1秒 | 免费 | ⭐⭐⭐⭐⭐ | 最快，结果已知 |

## 🔧 配置说明

### 场景1：节省成本模式

```bash
# 优先使用本地推理，大幅降低API成本
USE_LOCAL_INFERENCE=true
LOCAL_INFERENCE_FALLBACK=true

# 适用场景：
# - 预算有限
# - 客户端可以做分类映射
# - 对准确性要求不是特别高
```

### 场景2：高可用模式（推荐）

```bash
# 优先大模型，失败时降级到本地推理
USE_LOCAL_INFERENCE=false
LOCAL_INFERENCE_FALLBACK=true

# 适用场景：
# - 追求最佳准确性
# - 需要高可用性（大模型故障时不影响服务）
# - 可以接受API成本
```

### 场景3：纯大模型模式

```bash
# 只使用大模型，不降级
USE_LOCAL_INFERENCE=false
LOCAL_INFERENCE_FALLBACK=false

# 适用场景：
# - 对准确性要求极高
# - 不希望降级到本地推理
# - 大模型API稳定性有保障
```

### 场景4：纯本地推理模式

```bash
# 只使用本地推理，不使用大模型
USE_LOCAL_INFERENCE=true
LOCAL_INFERENCE_FALLBACK=false

# 适用场景：
# - 零成本运行
# - 隐私敏感，不希望上传到云端
# - 客户端可以完整实现分类映射
```

## 📝 API返回格式

### 大模型结果

```json
{
  "success": true,
  "data": {
    "category": "single_person",
    "confidence": 0.95,
    "description": "检测到单人照片",
    "local_inference_result": null
  },
  "from_cache": false,
  "processing_time_ms": 2500,
  "request_id": "req_xxx",
  "timestamp": "2025-10-11T12:00:00"
}
```

### 本地推理结果

```json
{
  "success": true,
  "data": {
    "category": "other",
    "confidence": 0.8,
    "description": "本地推理完成（需客户端映射分类）",
    "local_inference_result": {
      "success": true,
      "message": "模型推理完成",
      "idCardDetections": [...],
      "generalDetections": [...],
      "mobileNetV3Detections": {...}
    }
  },
  "from_cache": false,
  "processing_time_ms": 500,
  "request_id": "req_xxx",
  "timestamp": "2025-10-11T12:00:00"
}
```

**关键区别**：
- 本地推理时，`data.local_inference_result` 包含原始检测结果
- 客户端检查此字段，如果存在则使用 `MapObjectes2Category()` 做分类映射

## 🖥️ 管理后台控制

### 访问配置

1. 登录管理后台：http://123.57.68.4:8000
2. 点击 "⚙️ 配置管理" 标签页
3. 找到本地推理配置区域

### 配置选项

#### ✅ 使用本地推理

- **开启**：直接使用本地ONNX模型，不调用大模型API
- **关闭**（默认）：优先使用大模型API

#### ✅ 大模型失败时降级到本地推理

- **开启**（默认推荐）：提高服务可用性
- **关闭**：不降级，大模型失败就返回错误

### 注意事项

⚠️ **Web界面的配置只保存到浏览器**，实际生效需要：

1. 在服务器 `.env` 文件中添加配置：
   ```bash
   USE_LOCAL_INFERENCE=true
   LOCAL_INFERENCE_FALLBACK=true
   ```

2. 重启服务：
   ```bash
   systemctl restart image-classifier
   ```

## 🔄 客户端集成

### 检测并使用本地推理结果

```javascript
// 调用API
const response = await fetch('/api/v1/classify', {
    method: 'POST',
    body: formData
});

const result = await response.json();

// 检查是否使用了本地推理
if (result.data.local_inference_result) {
    // 使用本地推理结果
    const localResult = result.data.local_inference_result;
    
    // 客户端做分类映射
    const categoryId = await imageClassifierService.MapObjectes2Category(
        {
            idCard: localResult.idCardDetections,
            general: localResult.generalDetections,
            mobileNetV3: localResult.mobileNetV3Detections
        },
        imageUri,
        originalDimensions
    );
    
    console.log('最终分类:', categoryId);
} else {
    // 使用大模型结果
    console.log('分类:', result.data.category);
}
```

## 📊 推理方式标识

日志中会记录推理方式：

- `llm` - 大模型成功
- `local` - 本地推理成功（开关开启）
- `llm_fallback` - 本地推理失败后大模型成功
- `local_fallback` - 大模型失败后本地推理成功

## 🧪 测试

### 测试本地推理开关

```bash
# 1. 修改服务器配置
ssh root@123.57.68.4
cd /opt/ImageClassifierBackend
echo "USE_LOCAL_INFERENCE=true" >> .env
echo "LOCAL_INFERENCE_FALLBACK=true" >> .env
systemctl restart image-classifier

# 2. 测试API
curl -X POST "http://123.57.68.4:8000/api/v1/classify" \
  -F "image=@test.jpg" | jq .

# 3. 查看日志
journalctl -u image-classifier -f
```

## ✅ 优势总结

### 高可用性
- ✅ 大模型故障时自动降级
- ✅ 服务不中断
- ✅ 用户无感知切换

### 灵活性
- ✅ 可配置优先级
- ✅ 可选择是否降级
- ✅ Web界面可视化控制

### 成本优化
- ✅ 可完全使用本地推理（零API费用）
- ✅ 可混合使用（降低成本）
- ✅ 可纯大模型（最佳准确性）

### 用户体验
- ✅ 本地推理速度快（< 1秒）
- ✅ 大模型准确性高
- ✅ 降级策略保证可用性

## 📚 相关文档

- 📖 [本地模型推理使用说明](./本地模型推理使用说明.md)
- 📖 [架构说明：职责分离](./架构说明_服务器与客户端职责分离.md)
- 📖 [本地模型测试功能说明](./本地模型测试功能说明.md)

---

**功能版本**: 1.0.0  
**实现日期**: 2025-10-11  
**默认配置**: 优先大模型，失败时降级到本地推理  
**推荐配置**: `LOCAL_INFERENCE_FALLBACK=true` （高可用）

